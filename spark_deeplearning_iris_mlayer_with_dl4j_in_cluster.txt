import scala.collection.mutable.ListBuffer
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.canova.api.records.reader.RecordReader
import org.canova.api.records.reader.impl.CSVRecordReader
import org.deeplearning4j.nn.api.OptimizationAlgorithm
import org.deeplearning4j.nn.conf.MultiLayerConfiguration
import org.deeplearning4j.nn.conf.NeuralNetConfiguration
import org.deeplearning4j.nn.conf.layers.DenseLayer
import org.deeplearning4j.nn.conf.layers.OutputLayer
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.weights.WeightInit
import org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer
import org.nd4j.linalg.io.ClassPathResource
import org.nd4j.linalg.lossfunctions.LossFunctions

val recordReader:RecordReader = new CSVRecordReader(0,",")
//val conf = new SparkConf().setMaster("spark://master:7077").setAppName("FeedForwardNetwork-Iris")
//val sc = new SparkContext(conf)

val numInputs = 4
val outputNum = 3
val iterations = 100

val multiLayerConfig:MultiLayerConfiguration = new NeuralNetConfiguration.Builder().
 seed(12345).
 iterations(iterations).
 optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).
 learningRate(1e-1).
 l1(0.01).
 regularization(true).l2(1e-3).
 list(3).
 layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3).
   activation("tanh").
   weightInit(WeightInit.XAVIER).
   build()).
 layer(1, new DenseLayer.Builder().nIn(3).nOut(2).
   activation("tanh").
   weightInit(WeightInit.XAVIER).
   build()).
 layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).
   weightInit(WeightInit.XAVIER).
   activation("softmax").
   nIn(2).nOut(outputNum).build()).
 backprop(true).pretrain(false).
 build()

val network:MultiLayerNetwork = new MultiLayerNetwork(multiLayerConfig)
network.init
network.setUpdater(null)

val sparkNetwork:SparkDl4jMultiLayer = new SparkDl4jMultiLayer(sc,network)

val nEpochs = 5
val listBuffer = new ListBuffer[Array[Float]]()
(0 until nEpochs).foreach{i =>
  val net:MultiLayerNetwork = sparkNetwork.fit("file:///home/hadoop/iris.data",4,recordReader)
  listBuffer +=(net.params.data.asFloat().clone())
}
println("Parameters vs. iteration Output: ")
(0 until listBuffer.size).foreach{i => println(i+"\t"+listBuffer(i).mkString)}

